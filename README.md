# ğŸ›¡ï¸ Credit Card Fraud Detection

This project predicts whether a credit card transaction is **fraudulent or genuine** using machine learning models.  
We use the [Kaggle Credit Card Fraud Detection Dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud), which contains transactions made by European cardholders in September 2013.  

---

## ğŸ“‚ Project Structure

```
fraud-detection/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ creditcard.csv          # Dataset (downloaded from Kaggle)
â”‚
â”œâ”€â”€ preprocess.py               # Data loading & preprocessing
â”œâ”€â”€ train.py                    # Model definitions & training
â”œâ”€â”€ evaluate.py                 # Evaluation (confusion matrix, precision, recall, F1-score)
â”œâ”€â”€ main.py                     # Entry point (connects all modules)
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # Project documentation
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone <your-repo-url>
cd fraud-detection
```

### 2ï¸âƒ£ Create and activate a virtual environment
- **Windows**
  ```bash
  python -m venv venv
  venv\Scripts\activate
  ```
- **Mac/Linux**
  ```bash
  python -m venv venv
  source venv/bin/activate
  ```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add dataset
Download the dataset from Kaggle and place `creditcard.csv` inside the `data/` folder.

### 5ï¸âƒ£ Run the project
```bash
python main.py
```
## ğŸ§¾ Evaluation Metrics

### ğŸ”¹ Logistic Regression
Confusion Matrix:  
![Logistic Regression Confusion Matrix](img/LogisticRegression.png)

---

### ğŸ”¹ Random Forest
Confusion Matrix:  
![Random Forest Confusion Matrix](img/RandomForest.png)

---

### ğŸ”¹ Gradient Boosting
Confusion Matrix:  
![Gradient Boosting Confusion Matrix](img/GradientBoosting.png)

---

## ğŸ“Š Models Used
We trained and compared three machine learning models:

1. **Logistic Regression** â€“ interpretable baseline model  
2. **Random Forest** â€“ ensemble model with good accuracy  
3. **XGBoost** â€“ boosting-based classifier for handling imbalance  

---

## ğŸ§¾ Evaluation Metrics
We evaluate each model using:

- **Confusion Matrix**  
- **Precision**  
- **Recall**  
- **F1-Score**  

Example output for Logistic Regression:

```
=== Logistic Regression ===
Confusion Matrix:
 [[56863    31]
 [   54   104]]

              precision    recall  f1-score   support
           0     0.9991    0.9995    0.9993
           1     0.7704    0.6582    0.7102
```

- **Precision** â†’ Of all transactions predicted as fraud, how many were actually fraud.  
- **Recall (Sensitivity)** â†’ Of all actual fraud transactions, how many were correctly identified.  
- **F1-Score** â†’ Harmonic mean of precision & recall (balances the two).  

---

## ğŸ“Œ Requirements

See [requirements.txt](requirements.txt):

```
pandas
numpy
scikit-learn
matplotlib
seaborn
xgboost
```

Install them with:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ˆ Future Improvements
- Handle **class imbalance** with SMOTE or undersampling/oversampling.  
- Add **deep learning models** (e.g., ANN, LSTM).  
- Deploy as an **API** (Flask/FastAPI) for real-time fraud detection.  
- Add **ROC & Precision-Recall curve plots** for visual comparison.  

---

## ğŸ‘¨â€ğŸ’» Author
- **Your Name**  
- Student, SRM Institute of Science and Technology  
- Passionate about AI/ML, Computer Vision, NLP, and Cybersecurity  

---

## ğŸ“œ License
This project is for educational purposes. Dataset source: Kaggle.  
